{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of eid attribute: 6036\n",
      "Company Name: Amazon\n",
      "Value of eid attribute: 2763\n",
      "Company Name: Deloitte\n",
      "Value of eid attribute: 715\n",
      "Company Name: Walmart\n",
      "Value of eid attribute: 432\n",
      "Company Name: McDonald's\n",
      "Value of eid attribute: 194\n",
      "Company Name: Target\n",
      "Value of eid attribute: 7927\n",
      "Company Name: Infosys\n",
      "Value of eid attribute: 4138\n",
      "Company Name: Accenture\n",
      "Value of eid attribute: 354\n",
      "Company Name: IBM\n",
      "Value of eid attribute: 1651\n",
      "Company Name: Microsoft\n",
      "Value of eid attribute: 2202\n",
      "Company Name: Starbucks\n",
      "{'Amazon': '6036', 'Deloitte': '2763', 'Walmart': '715', \"McDonald's\": '432', 'Target': '194', 'Infosys': '7927', 'Accenture': '4138', 'IBM': '354', 'Microsoft': '1651', 'Starbucks': '2202'}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "driver_path = 'D:\\Info Retrieval\\chromedriver.exe'\n",
    "chrome_service = webdriver.ChromeService(executable_path=driver_path)\n",
    "driver=webdriver.Chrome(service=chrome_service)\n",
    "\n",
    "company_links = []\n",
    "companies = {}\n",
    "\n",
    "target_url = \"https://www.glassdoor.com/Explore/browse-companies.htm?industry=10012&country=SG&overall_rating_low=2.5&page=\"\n",
    "num_pages = 1 # Number of pages to scrape -- CHANGE THIS TO THE AMOUNT REQUIRED\n",
    "\n",
    "nums = list(range(1, num_pages+1))\n",
    "url_mains = list(map(lambda n: target_url + str(n), nums))\n",
    "\n",
    "count = 0\n",
    "\n",
    "for url in url_mains:\n",
    "    count += 1\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    elems = driver.find_elements(By.CLASS_NAME, \"col-md-8\")\n",
    "    # print(elems)\n",
    "    for elem in elems:\n",
    "        # print(\"Text:\", elem.text)\n",
    "        inner_html = elem.get_attribute('innerHTML')\n",
    "        # print(\"Secondary:\", inner_html)\n",
    "\n",
    "        soup = BeautifulSoup(inner_html, 'html.parser')\n",
    "\n",
    "        # Add Company to dictionary\n",
    "        outer_divs = soup.find_all('div', class_='mt-0 mb-std p-std css-mdw3bo css-errlgf')\n",
    "        for outer_div in outer_divs:\n",
    "            # Retrieve the value of the eid attribute\n",
    "            eid_value = outer_div.get('data-brandviews').split('=')[-1]\n",
    "\n",
    "            # Retrieve the value of employer id of the company\n",
    "            print(\"Value of eid attribute:\", eid_value)\n",
    "            \n",
    "            company_name = outer_div.find('h2').text\n",
    "            print(\"Company Name:\", company_name)\n",
    "            companies[company_name] = eid_value\n",
    "\n",
    "        print(companies)\n",
    "    # response = driver.page_source\n",
    "    if count % 3 == 0:\n",
    "        driver.close()\n",
    "        driver=webdriver.Chrome(service=chrome_service)\n",
    "driver.close()\n",
    "\n",
    "# Save the dictionary to a CSV file\n",
    "df = pd.DataFrame(list(companies.items()),columns = ['Company Name','Employer ID'])\n",
    "df.to_csv('companies.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.glassdoor.com/Reviews/Amazon-Reviews-E6036_P1.htm?filter.iso3Language=eng\n",
      "Company Name: Amazon\n",
      "EID: 6036\n",
      "https://www.glassdoor.com/Reviews/Amazon-Reviews-E6036_P2.htm?filter.iso3Language=eng\n",
      "Company Name: Amazon\n",
      "EID: 6036\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def get_company_links(company_name:str, eid:int, page:int=1):\n",
    "    return f\"https://www.glassdoor.com/Reviews/{company_name}-Reviews-E{eid}_P{page}.htm?filter.iso3Language=eng\"\n",
    "\n",
    "# Initialize the Chrome driver\n",
    "driver_path = 'D:\\Info Retrieval\\chromedriver.exe'\n",
    "chrome_service = webdriver.ChromeService(executable_path=driver_path)\n",
    "driver=webdriver.Chrome(service=chrome_service)\n",
    "\n",
    "try:\n",
    "    companies.items()\n",
    "except:\n",
    "    df = pd.read_csv('companies.csv')\n",
    "    companies = dict(zip(df['Company Name'], df['Employer ID']))\n",
    "\n",
    "for key, value in companies.items():\n",
    "    # Initialize lists to store the data\n",
    "    OverallRatings = []\n",
    "    ReviewDates = []\n",
    "    ReviewTitles = []\n",
    "    JobTitles = []\n",
    "    JobDetails = []\n",
    "    Locations = []\n",
    "    Pros = []\n",
    "    Cons = []\n",
    "\n",
    "    max_pages = 2\n",
    "    for page in range(1, max_pages+1):\n",
    "\n",
    "        url = get_company_links(key, value, page)\n",
    "        print(url)\n",
    "        print(\"Company Name:\", key, \"EID:\", value, \"Page:\", page)\n",
    "        driver.get(url)\n",
    "        timer = random.randint(5, 10)\n",
    "        time.sleep(timer)\n",
    "        \n",
    "        elem = driver.find_elements(By.ID, \"ReviewsRef\")[0]\n",
    "\n",
    "        # print(\"Text:\", elem.text)\n",
    "        inner_html = elem.get_attribute('innerHTML')\n",
    "        # print(\"Secondary:\", inner_html)\n",
    "\n",
    "        soup = BeautifulSoup(inner_html, 'html.parser')\n",
    "\n",
    "        # print(soup.prettify())\n",
    "        # Find the list item with class 'noBorder'\n",
    "        list_items = soup.find_all('li', class_='noBorder')\n",
    "\n",
    "        for list_item in list_items:\n",
    "\n",
    "            # Retrieve the overall rating\n",
    "            overallRating = list_item.find('span', class_=\"review-details__review-details-module__overallRating\").text\n",
    "            OverallRatings.append(overallRating)\n",
    "            # print(\"OverallRating:\", overallRating)\n",
    "\n",
    "            # Retrieve the review date\n",
    "            reviewDate = list_item.find('span', class_=\"review-details__review-details-module__reviewDate\").text\n",
    "            ReviewDates.append(reviewDate)\n",
    "            # print(\"ReviewDate:\", reviewDate)\n",
    "\n",
    "            # Retrieve the review title\n",
    "            reviewTitle = list_item.find('a', class_=\"review-details__review-details-module__detailsLink review-details__review-details-module__title\").text\n",
    "            ReviewTitles.append(reviewTitle)\n",
    "            # print(\"ReviewTitle:\", reviewTitle)\n",
    "\n",
    "            # Retrieve the reviewer's job title\n",
    "            jobTitle = list_item.find('span', class_=\"review-details__review-details-module__employee\").text\n",
    "            JobTitles.append(jobTitle)\n",
    "            # print(\"JobTitle:\", jobTitle)\n",
    "\n",
    "            # Retrieve the reviewer's job details\n",
    "            jobDetails = list_item.find('div', class_=\"review-details__review-details-module__employeeDetails\").text\n",
    "            JobDetails.append(jobDetails)\n",
    "            # print(\"JobDetails:\", jobDetails)\n",
    "\n",
    "            # Retrieve the reviewer's location\n",
    "            try:\n",
    "                location = list_item.find('span', class_=\"review-details__review-details-module__location\").text\n",
    "            except:\n",
    "                location = None\n",
    "            Locations.append(location)\n",
    "            # print(\"Location:\", location)\n",
    "\n",
    "            # Retrieve the review pros\n",
    "            pros_and_cons = list_item.find_all('p', class_=\"review-details__review-details-module__isCollapsed\")\n",
    "            if len(pros_and_cons) < 2:\n",
    "                pros_and_cons = list_item.find_all('p', class_=\"review-details__review-details-module__isExpanded\")\n",
    "            pros = pros_and_cons[0].text\n",
    "            Pros.append(pros)\n",
    "            # print(\"Pros:\", pros)\n",
    "\n",
    "            # Retrieve the review cons\n",
    "            cons = pros_and_cons[1].text\n",
    "            Cons.append(cons)\n",
    "            # print(\"Cons:\", cons)\n",
    "        \n",
    "            # print(list_item.prettify() )\n",
    "            # reviews_feed = soup.find_all('div', id='ReviewsFeed')\n",
    "            # print(len(reviews_feed.find('li', class_='noBorder')))\n",
    "\n",
    "    # response = driver.page_source\n",
    "            if page % 3 == 0:\n",
    "                driver.close()\n",
    "                driver=webdriver.Chrome(service=chrome_service)\n",
    "\n",
    "    # response = driver.page_source\n",
    "    # Put the data into the dataframe\n",
    "    company_list = [key] * len(OverallRatings)\n",
    "    Reviews = pd.DataFrame(list(zip(company_list, OverallRatings, ReviewDates, ReviewTitles, JobTitles, JobDetails, Locations, Pros, Cons)), \n",
    "                              columns = ['Company Name', 'Overall Rating', 'Review Date', 'Review Title', 'Job Title', 'Job Details', 'Location', 'Pros', 'Cons'])\n",
    "    break\n",
    "\n",
    "Reviews.to_csv('reviews.csv', index=False, encoding='utf-8')\n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inforet",
   "language": "python",
   "name": "inforet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
